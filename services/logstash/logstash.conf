# 数据采集规则
input {
    # nginx 日志采集配置
    file {
        type => "local_error"           # 可以自行定义，方便后面判断，但是不要使用大写，否则报错
        start_position => "beginning"   # 从日志其实位置采集
        stat_interval => "3"            # 采集频率为 3 秒
        codec => plain                  # plain 表示采集的数据是 文本格式，非 json
        path => "/data/logs/nginx/nginx.localhost.error.log"
    }
}

filter {
    if [type] == "local_error" {
        mutate {
            # 指定文件名，可改为 localerr
            add_field => {"[@metadata][target_index]" => "localerr-%{+YYYY.MM.dd}"}
        }

        # # 自定义字段用
        # grok {
        #     match => {
        #         "message" => "(?<created_at>%{YEAR}[./-]%{MONTHNUM2}[./-]%{MONTHDAY} %{TIME:time2}) \[%{WORD:errLevel}]  (?<errMsg>([\w\W])*), client\: %{IP:clientIp}(, server\: %{IPORHOST:server})?(, request\: \"%{DATA:request}\")?(, upstream\: \"%{DATA:upstream}\")?(, host\: \"%{DATA:host}\")?"
        #     }
        # }
    }

    # 删除一些多余字段
    mutate {
        remove_field => ["@version"]
    }
}

output {
    # 将最终处理的结果输出到调试面板（控制台），您可以开启，先观察处理结果是否是您期待的，确保正确之后，注释掉即可
    # stdout { codec => rubydebug }

    # 官方说，这里每出现一个 elasticsearch 都是一个数据库客户端连接，建议用一个连接一次性输出多个日志内容到 elk ，像如下这样
    # 这样配置可以最大减少 elk 服务器的连接数，减小压力，因为 elk 今后将管理所有项目的日志，数据处理压力会非常大  
    elasticsearch  {
        hosts => ["http://elasticsearch:9200"]
        index => "%{[@metadata][target_index]}"
    }
}